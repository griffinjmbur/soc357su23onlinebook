

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>basic math for social sciences I: basic descriptive and inferential statistics &#8212; SOC357</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '2023-07-01_SOC357su23_basic_math_ch_1';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Course parameters" href="2023-06-16_SOC357su23_syllabus.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/SOC357_website_logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/SOC357_website_logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to SOC357
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2023-06-16_SOC357su23_syllabus.html">Course parameters</a></li>









<li class="toctree-l1 current active"><a class="current reference internal" href="#">basic math chapter 1</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F2023-07-01_SOC357su23_basic_math_ch_1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/2023-07-01_SOC357su23_basic_math_ch_1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>basic math for social sciences I: basic descriptive and inferential statistics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-basic-operations">Some basic operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributions-regular-variable-and-sampling">Distributions: regular variable and sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-distributions-cumulative-distribution-and-probability-mass-functions">Discrete distributions: cumulative distribution and probability mass functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-distributions-probability-density-functions">Continuous distributions: probability density functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-inference">Basics of inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-extremely-useful-alternative-expression-for-the-variance">An extremely useful alternative expression for the variance.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-formula-for-the-variance-of-a-sum-of-random-variables">The general formula for the variance of a sum of random variables.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-surprising-demonstration-of-any-element-s-probability-of-inclusion-when-sampling-from-a-finite-population-without-replacement">The surprising demonstration of any element’s probability of inclusion when sampling from a finite population without replacement</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="basic-math-for-social-sciences-i-basic-descriptive-and-inferential-statistics">
<h1>basic math for social sciences I: basic descriptive and inferential statistics<a class="headerlink" href="#basic-math-for-social-sciences-i-basic-descriptive-and-inferential-statistics" title="Permalink to this heading">#</a></h1>
<section id="some-basic-operations">
<h2>Some basic operations<a class="headerlink" href="#some-basic-operations" title="Permalink to this heading">#</a></h2>
<ol class="arabic">
<li><p>Suppose you run across an expression such as <span class="math notranslate nohighlight">\(\sum_{i=1}^n y_i\)</span>. What does this mean? <strong>The summation operator, <span class="math notranslate nohighlight">\(\sum\)</span></strong>, tells us to sum from the observation denoted at the bottom of the capital greek letter <span class="math notranslate nohighlight">\(\Sigma\)</span> (“sigma”, which makes the “s” sound, as in <strong>s</strong>um) and go up until the observation denoted on top of the <span class="math notranslate nohighlight">\(\Sigma\)</span>. For us, the bottom item will almost always be individual (denoted <em>i</em>) equal to one, going up until individual equal to <em>n</em>, where <em>n</em> is the sample size. Please note carefully that we’re using small Latin letters here in two different ways: <em>i</em> is an index, which varies across individuals; <em>n</em> is a constant and a property of the data-set as a whole. Examine this simple example data-set:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \begin{array}{|c|c|}
        \hline
            \textbf{i} &amp; \textbf{y} \\
        \hline
            \text{1} &amp; \text{3} \\
        \hline
            \text{2} &amp; \text{10} \\
        \hline
            \text{3} &amp; \text{2} \\
        \hline
            \text{4} &amp; \text{4.5} \\
        \hline
    \end{array}
    \end{split}\]</div>
<p>Now, try to carry out the arithmetic operation indicated by the following notation: <span class="math notranslate nohighlight">\(\sum_{i=1}^n 
 y_i\)</span>.<a class="footnote-reference brackets" href="#answer" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p>
<p>The numbering of individuals is, for our purposes, basically always arbitrary, and summation is
commutative anyways (the order of the items summed, or the <em>summands</em>, does not matter), so sometimes you
will see the sub- and superscripts omitted.</p>
<p>Now let’s define the <em>sample total</em>, which allows us to estimate the population total. We write the total
with the use of sample weights, which are not a key feature of this class, but they are something that you
should know the basics of. Our weights are simply the inverse probability of selection into the
sample. This is because the total of the population is (unlike the mean, with which we’ll work more often)
a function of the size of the population, of which the sample is an arbitrarily-sized fragment.
Note that if each observation has a different probability, we need to put this inside of the summation.
In our case,
we’ll deal only with the equal probability of selection (<em>epsem</em>) method, so this is a constant that can
be factored out.</p>
<p>The last piece of notational throat-clearing that we need at the momen is that
we will need is that we we will represent random variables in their most
abstract form with capital Roman letters; their sample equivalents have lowercase Roman letters. Greek
letters are reserved for <em>parameters</em>, facts about the population that we want to know (but can’t directly
observe). Here, the <span class="math notranslate nohighlight">\(\tau\)</span> (“tau”, which makes a “t” sound) stands for “population total”. The notation
means that to find the sample estimate of the
total of the random variable <em>Y</em>, we sum up the observed values in the sample, <span class="math notranslate nohighlight">\(y_i\)</span>. Note that this also
means that <span class="math notranslate nohighlight">\(\pi\)</span> will typically mean, in our context, the <em>probability</em> of something (here, selection into
the sample) rather than Archimedes’ constant (3.1415926…)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
        \widehat{\tau}_Y &amp;= \frac{1}{\pi} \sum_{i=1}^n y_i
    \end{align*}\]</div>
</li>
<li><p><strong>The mean</strong> is a measure of central tendency—something like the “characteristic value of a distribution”, which the mean, median and mode all get at. We focus most attention on the mean in this class; however, the median is an important measure, especially in the context of quantiles more generally. <strong>“Mean” is simply the formal term for the arithmetic average.</strong> We’ll focus on the simple case where possible outcomes are discrete, meaning that they are finite in number or <strong>countably</strong> infinite (more in a second); simple descriptive statistics for any real world data-set can be calculated using these simpler discrete formulae. Sometimes, we will need to discuss variables with infinite possible values, known as continuous variables.</p>
<p>For example, if we record the number of cars someone has, while they <em>could</em> tell us that they have
any number up to infinity, this is <em>countably</em> infinite; setting aside the formal definition, this means
basically that if I pick an arbitrary number of functioning cars someone owns (say, five cars), I can tell
you the next value in the set of possible outcomes with no question (six cars); so, cars owned is a
discrete variable.</p>
<p>By contrast, if we ask someone about their
height and they reply with “74 inches”, I can’t say what height is “next”. I could, for example, say 75
inches, but since 74 inches is about 188cm, 189cm is nearer than 75 inches. And this, of course, can
extend even further, to the micron (if we had a microscope). Since height can take on an uncountable
number of possible values: it is continuous. (It is fun, but not super important, to wonder about why some
variables are continuous and others are discrete. In the cars case, it is just definitional; while parts
of cars do exist, if I am only interested in complete, functioning cars, those simply do not come in
halves; by contrast, there is no similar limit on height).</p>
<p><strong>The mean is sometimes referred to as the expected value or expectation.</strong> When we write the mean as an
expected value or expectation, we conceptualize it as a population-level property; it is the weighted sum
of all possible values of the variable. So, we are summing over the possible outcomes of the variable when
we write the mean as an expectation—we are <strong>not</strong> summing over any observed individuals.
Many textbooks and internet sources do not change their notation here, which confuses new students, so I
will now re-index the sum. Let <em>k</em> index an outcome of the random variable <em>Y</em> (i.e., “<em>k</em>=5” would mean
the fifth possible outcome, where order again typically does not matter) and let <em>K</em> indicate the number
of possible outcomes. Then, for a <em>discrete</em> random variable (don’t worry about the continuous formula
here):</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb{E}[Y] &amp;= \sum_{k=1}^K y_k * \mathbb{P}(Y=y_k)
    \end{align*}\]</div>
<p>This says that we look at each possible outcome <em>k</em>, starting with <em>k</em> = 1, and multiple the value of that
outcome <span class="math notranslate nohighlight">\(y_k\)</span> by its probability of occurring, <span class="math notranslate nohighlight">\(\mathbb{P}(Y=y_k)\)</span>. Note, also, that <span class="math notranslate nohighlight">\(\mu_Y\)</span> is another
way to denote the population mean; the difference between this and <span class="math notranslate nohighlight">\(\mathbb{E}[Y]\)</span> is like the difference
between writing “the average” and “total over sample size”, respectively.</p>
<p>For a given <em>set</em> of data, the formula does not change, even if the variable is itself continuous; this is
because all actual observed data-sets are discrete: there is a finite number of observed values. If we
have any repeated values, we effectively use this as a sample estimate of the probability of observing
that value at  the population level. If the variable itself has many possible outcomes (whether it is
discrete or continuous) and no values repeat themselves exactly, then the estimated probability of each
value is just <span class="math notranslate nohighlight">\(\frac{1}{n}\)</span>.</p>
<p>So, in practice, with equal probability sampling, it is often simpler to write out the mean as the simple
arithmetic mean, even if we have repeat values.</p>
<p>Note one exception for our notation rules mentioned above: It would make <em>most</em> sense to use, for the
sample mean, the notation <span class="math notranslate nohighlight">\(\widehat{\mu}_Y\)</span>. However, most textbooks use the notation <span class="math notranslate nohighlight">\(\bar{Y}\)</span> for the
random variable and <span class="math notranslate nohighlight">\(\bar{y}\)</span> for some specific sample mean. The problem with this notation is that it
breaks the general rule about Greek and Roman letters mentioned above and requires you to  memorize a new
notational rule, but it is extremely common, so we will use it. For any specific sample mean…</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \bar{y} &amp;= \sum_{i=1}^n \frac{1}{n} * y_i \cr
    \bar{y} &amp;=  \frac{1}{n} \sum_{i=1}^n y_i  \
        &amp;&amp;  \text{since $\frac{1}{n}$ is constant, it factors out of the sum} \cr
    \end{align*}\]</div>
<p>Note that this means that a <em>simple</em> mean implicitly takes the inclusion of each member of the sample to
have been equally probable. This is false with a stratified random sample. So, we fix this by multiplying
by the inverse of the probability of inclusion, the <em>sample weight</em> <span class="math notranslate nohighlight">\(w = \frac{1}{\pi_i}\)</span>, where <span class="math notranslate nohighlight">\(\pi_i\)</span>
is <em>i</em>’s probability of being included in the sample as <span class="math notranslate nohighlight">\(\pi_i\)</span>. Note that <span class="math notranslate nohighlight">\(\pi_i\)</span> is a parameter, not a
statistic; every member of our sample has a <em>known</em> probability of inclusion.
In the case of simple random sampling, every member of the population has an equal probability of
inclusion of <span class="math notranslate nohighlight">\(\frac{n}{N}\)</span>, where <em>n</em> is the sample size and <em>N</em> is the population size.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
     \bar{y} &amp;= \frac{1}{N} \sum_{i=1}^n \frac{1}{\pi_i} * y_i \cr
    &amp;= \frac{1}{N} \sum_{i=1}^n \frac{N}{n} * y_i &amp;&amp; \text{in this case, $\pi_i$ = $\frac{n}{N}$} \cr
    &amp;= \frac{1}{N} * \frac{N}{n} \sum_{i=1}^n y_i &amp;&amp; \text{constants factor out of sums} \cr
    &amp;= \frac{1}{n}\sum_{i=1}^n y_i
    \end{align*}\]</div>
<p>Note that this formula is helpful because is can be extended to cases where we don’t have equal
probabilities of inclusion.</p>
<p>For some practice, try calculating the sample mean for the data-set given above.<a class="footnote-reference brackets" href="#answer3" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p>
<p>By the way, note also one important fact: means do <em>not</em> make sense for nominal variables (such as race) and should be taken with a large grain of salt for ordinal variables.</p>
<p>What should we do we such variables in order to calculate meaningful descriptive statistics? We have two alternatives. First, you can simply calculate the estimated probability mass function for the variables, which is a fancy way of saying “list the percent of people in the sample who take on each value”, e.g. the (weighted) GSS 2018 has about 72 percent white respondents, 15 percent black respondents, and 13 percent “other” respondents (forgive the use of this crude category).</p>
<p>Note also that, if we considered these three separate binary variables, where we mark people as 1s if they are members of that group and zeros if not, we could also consider each of these percentages the mean of each variable. E.g.,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \overline{white} &amp;= \sum_{i=1}^n \frac{1}{n} * white_i \cr
    &amp;= \frac{1}{n} * (\underbrace{1 + 1 + ... + 1}_{n_{\text{whites}}} \
    + \underbrace{0 + 0 + ... 0}_{n_{\text{non-whites}}}) \cr
    &amp;= \frac{n_{\text{whites}}}{n} &amp;&amp; \text{Note that this is the proportion of whites in the sample}
    \end{align*}\]</div>
</li>
<li><p><strong>The variance</strong> is a measure of spread or dispersion. You can think about it as the expected difference between a randomly-selected observation and its mean. We define it formally at the population-level as the expectation of the squared difference between the random variable <em>Y</em> and its mean <span class="math notranslate nohighlight">\(\mu_Y: \mathbb{V}[Y] = \mathbb{E}[(Y - \mu)^2]\)</span>. You may wonder why we square the difference. There is a very good reason for this that leads us to the proof of a useful fact about the mean, which is that it is the value for a given set about which all deviations are not just minimized but also zero!</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    &amp;1. \mathbb{E}[Y - \mu_Y] = \mathbb{E}[Y] - \mathbb{E}[\mu_Y] 
                &amp;&amp; \text{expectation operator is linear} \cr
    &amp;2. \mathbb{E}[Y] = \mu_Y &amp;&amp; \text{by definition} \cr
    &amp;3. \mathbb{E}[\mu_Y] = \mu_Y &amp;&amp; \text{expectation of a constant is just that constant} \cr
    &amp;4. \mathbb{E}[Y - \mu_Y] = \mu_Y - \mu_Y = 0  &amp;&amp; \text{arithmetic}
        \end{align*}\]</div>
<p>It is useful to think about the variance operator, <span class="math notranslate nohighlight">\(\mathbb{V}\)</span>, as a function that produces the variance, and the actual variance itself as a fixed quantity for a given population. We will denote this <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Do not let yourself get frustrated by the power of 2. The reason we do this is quite simple: the variance is easier to <em>calculate</em> with, but the interpretation—the expected squared distance of a point from the mean, or the average squared distance of a point from the mean—is more difficult. One solution is to just take the root of the whole thing, which we then call the standard deviation, which is <span class="math notranslate nohighlight">\(\sigma\)</span> for the population and <em>s</em> in the sample (the sample variance is then <em>s</em>^2).</p>
<p>The sample variance formula is also straightforward, with the exception of the denominator. Don’t worry too much about this; it is called the Bessel correction, and it fixes the bias caused by the fact that we estimate the population mean with the sample mean, which understands the true variance (the spread of the points will generally be closer to the sample mean than the population mean). But, the variance is still fundamentally the mean squared deviation.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
        s^2 = \frac{\sum_{i=1}^n (y_i - \bar{y})^2}{n-1} \cr
        s = \sqrt{\frac{\sum_{i=1}^n (y_i - \bar{y})^2}{n-1}}
    \end{align*}\]</div>
<p>Finally, try to calculate the sample variance of our data-set above. <a class="footnote-reference brackets" href="#answer4" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></a></p>
</li>
</ol>
</section>
<section id="distributions-regular-variable-and-sampling">
<h2>Distributions: regular variable and sampling<a class="headerlink" href="#distributions-regular-variable-and-sampling" title="Permalink to this heading">#</a></h2>
<section id="discrete-distributions-cumulative-distribution-and-probability-mass-functions">
<h3>Discrete distributions: cumulative distribution and probability mass functions<a class="headerlink" href="#discrete-distributions-cumulative-distribution-and-probability-mass-functions" title="Permalink to this heading">#</a></h3>
<p>We saw above some methods of characterizing distributions by their parameters (their mean and standard deviation), but what if we want to examine the distribution overall? We need to look at this separately for discrete and continuous variables. We’ll start with what I’m calling <em>regular variable</em> distributions, distributions that characterize an actual variable of interest, such as mother’s education or income or age; then, we’ll turn to <em>sampling</em> distributions, distributions that characterize the behavior of a sample statistic over many different samples. This second quantity is of interest to us for <em>inferential reasons</em>; we care about income, but to make inference about income, we need to understand the behavior of sample means of income across many samples. This difference is <em>extremely</em> important, and in many years of teaching this material, I have found that the most common mistake students make is to ignore this distinction or to realize that one does not understand it but to be excessively easygoing about this fact. You <em>must</em> make sure that you understand this difference.</p>
<p>Let’s start with regular variable distributions. These are the distributions that we typically <em>directly</em> care about: they give us information about different levels of our variable in the population of interest or a sample thereof, whereas sampling distributions are primarily useful for <em>analytical</em> reasons.</p>
<p>There are two useful ways to examine an individual distribution for discrete variables. The first is a cumulative distribution function (CDF), which plots on the X-axis the value of the variable and on the Y-axis, the cumulative percent (i.e., probability) of data observed up to that value. A CDF makes sense for both continuous and discrete random variables. The height of this function is the percent of observations less than or equal to some value of the variable; remembering that percent and probabilities usually are the same thing, and letting <span class="math notranslate nohighlight">\(\mathbb{P}\)</span> mean “probability”, the height of this function <span class="math notranslate nohighlight">\(F(X) = \mathbb{P}(X \leq x)\)</span>.</p>
<p>The second is a probability mass function (PMF), which plots possible values of the variable on the X-axis (again), but this time plots the probability of observations <em>just for that value</em> on the Y-axis. You can see these two approaches belows. Notice that a PMF is basically just a histogram. The height of this function is the percent of observations <em>exactly</em> equal to some value of the variable; the height of this function <span class="math notranslate nohighlight">\(F(X) = \mathbb{P}(X = x)\)</span>.</p>
<p><img alt="CDF and PMF" src="_images/pmfcdfnumkids.png" /></p>
</section>
<section id="continuous-distributions-probability-density-functions">
<h3>Continuous distributions: probability density functions<a class="headerlink" href="#continuous-distributions-probability-density-functions" title="Permalink to this heading">#</a></h3>
<p>For continuous variables, we can also generate cumulative distribution functions, but probability mass functions don’t, strictly speaking, make sense. This is because the probability of any exact value of such a variable is zero. This is surprising at first, but if you have trouble buying this, imagine the probability of any specific value of a variable which could possibly be equal to, say, <span class="math notranslate nohighlight">\(\pi\)</span> or <em>e</em> (i.e., irrational numbers)—if it’s not zero, it’s extremely small, right?</p>
<p>It is important to note that <em>all</em> actual, observed populations are, practically speaking, “discrete” in that the number of outcomes is finite. So, while a variable can have a PDF, any dataset of <em>realizations</em> cannot; we just have an estimate of the PDF, which is actually a PMF. For example, here is the empirical CDF and PMF of height from the 2017-18 NHANES. Height is a continuous variable, but the realization of it in this sample is discrete, even if it is massively less-discrete than the number of children seen above. If you look carefully, you can see the slight “kinks” in the line; these are not problems of pixellation but rather that this is an empirical estimate of continuous function.</p>
<p><img alt="CDF and PMF for realization of continuous variable" src="_images/height_CDF_P'D'F.png" /></p>
<p>What do actual density functions look like and represent, then? Cumulative distribution functions look nearly the same, and the function <span class="math notranslate nohighlight">\(F(X)\)</span> means the same thing: the probability that the variable takes on value less than or equal to some value X; formally <span class="math notranslate nohighlight">\(\mathbb{P}(X \leq x)\)</span> (recall that uppercase Roman letters mean random variables and lowercase Roman letters mean realizations or trials of that random variable). <em>Density</em> is a trickier concept; what it means formally is <em>probability per unit</em>. You should think about density as the “speed of probability”; the PDF is just a graph of the <em>speed</em> of the CDF (formally, the PDF is the simple first derivative of the CDF). So, note that the graph of the PDF is <em>tallest</em> where the slope of the CDF is <em>steepest</em>.</p>
<p>Importantly, this means that the <em>height</em> of the PDF is not of direct interest, both because the height is hard to interpret intuitively, even if the mathematical definition is simple; and, at any rate, . What we care about instead are <em>areas under the curve</em> between two points <em>a</em> and <em>b</em>, which <em>are</em> probabilities (not densities) that the variable will take on a value between <em>a</em> and <em>b</em>.<a class="footnote-reference brackets" href="#fn-auc" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p>
<p>So, what are PDFs good for? Well, we can use them to model the probability distribution of a random variable. Remarkably, it turns out that the random variable <span class="math notranslate nohighlight">\(\overline{Y}\)</span> is 1) continuous and 2) Normally-distributed, even if the distribution of the random variable <em>Y</em> is far from Normally-distributed—this is the power of the Central Limit Theorem. Here is a simulation we’ve seen before in this class, using a large number of replications (so, this is not the true PDF, but it gives us a clear sense of what that would look like). Further, we happen to know its</p>
<p><img alt="The power of the central limit theorem" src="_images/CLT_power.png" /></p>
<p>Here are the Standard Normal CDF and PDF, which characterize the distribution of standardized sample means. All sample means are Normally-distributed, but they have arbitrary means and standard deviations; subtracting the mean and dividing by the standard deviation gives a standard score <span class="math notranslate nohighlight">\(z = \frac{\overline{y} - \mu_{\overline{Y}}}{\sigma_{\overline{Y}}}\)</span>. We typically won’t actually know the mean and standard deviation, but I’ll address that in a little bit.</p>
<p><img alt="Standard Normal CDF and PDF" src="_images/Normal_CDF_PDF.png" /></p>
<p>Now, it is worth noting that although it is difficult to take the areas under the curve (AUC) of the Normal by hand with calculus, it is easy to do so with a computer or a table. Further, the AUCs associated with +/- one, two, and three standard deviations are easy to memorize. In particular, the fact that going out exactly 1.96 standard deviations includes almost exactly 95 percent AUC is very easy to remember, and if you’ve heard of a 95 percent confidence interval (which is extremely standard, though technically arbitrary), this is why.</p>
<p><img alt="Areas under the curve of the NormalPDF" src="_images/Normal_PDF_AUCs.png" /></p>
<p>If the population is sufficiently large relative to the the size of the sample, each individual observation can be treated as independent of the others and taken from the same distribution (so long as they are drawn at random): they are “independent, identically-distributed” (IID) random variables.</p>
<p>This concept is often very challenging for novices, so I want you to go back and re-read the preceding paragraph. <em>Each person in our sample, before they are actually selected, is a random variable</em>. For example, if I plan to sample 500 people and ask them their income, but I have not yet done so, those 500 hypothetical-people represent 500 random variables, all taken from the same population distribution of income. It is often helpful to consider the analogy to an experiment in which we flip coins: the concept of “the outcome of the 10th coin I flip” is more-obviously a random variable for the simple reason that it hasn’t yet happened. While the person whose income I ultimately observe in the sample has already <em>obtained</em> that income, there is a point at which I don’t know <em>which</em> person I’ll observe, and thus that “person’s” income is rather like the outcome of the as-yet-unflipped coin.</p>
<p>Here is the extremely important fact about that. When we do inference about a population, we cannot observe it, and we do not know its distribution; its distribution likely does not fit some exact mathematical function anyways (and it may not be well-approximated by one). So, observing the realization of <em>one</em> random variable—taking one person out of the population and asking, say, their income—is not very informative. When we turn a set of random variables into a total or a mean, however, we know that the shape of the distribution <em>they</em> come from is not the (unknown) “regular variable” distribution but the sampling distribution. <em>This is the primary reason why (pre-super computer) statistics is even possible</em>. It is such a striking fact that Galton claimed that “<a class="reference external" href="https://galton.org/cgi-bin/searchImages/galton/search/books/natural-inheritance/pages/natural-inheritance_0073.htm">[t]he law would have been personified by the Greeks and deified, if they had known of it</a>”.</p>
</section>
<section id="basics-of-inference">
<h3>Basics of inference<a class="headerlink" href="#basics-of-inference" title="Permalink to this heading">#</a></h3>
<p>So, we can now do inference! All we need to do is find the standard score of a sample mean and then find the associated AUC. Recall before that sample means are unbiased estimators for simple random samples, so our mean of the sampling distribution, <span class="math notranslate nohighlight">\(\mu_{\overline{Y}}\)</span>, is simply the population mean, <span class="math notranslate nohighlight">\(\mu_Y\)</span>.</p>
<p>Now, we are after the standard deviation of the sample mean, which is referred to as the standard error for the sake of distinguishing it from the “regular variable” standard deviation. It turns out that it is much easier to derive the variance and then simply take the root. Note that we are after the variance of a sum of IID random variables (divided by the constant <em>n</em>).  While the extremely useful general formula for the variance of a sum of random variables is slightly complex (see appendix), it is not just extremely useful but extremely simple for independent random variables: it is just the sum of their variances. Let <span class="math notranslate nohighlight">\(\overline{Y}\)</span> represent the sample mean of a random variable comprised of <em>n</em> IID random variables <span class="math notranslate nohighlight">\(Y_1, Y_2, ... Y_n\)</span>; each variable comes from a population distribution with standard deviation <span class="math notranslate nohighlight">\(\sigma_Y\)</span>, which we’ll simply refer to using <span class="math notranslate nohighlight">\(\sigma\)</span> for simplicity.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \mathbb{V}[\overline{Y}] = \sigma^2_{\overline{Y}} &amp;= \mathbb{V}[\sum_{i=1}^n \frac{Y_i}{n}] \cr
    &amp;= \frac{1}{n^2} \mathbb{V}[\sum_{i=1}^n  Y_i] \
        &amp;&amp; \text{constants factor out of variances as squares} \cr
    &amp;= \frac{1}{n^2} (\sigma^2_1 + \sigma^2_2 + ... + \sigma^2_n) \
        &amp;&amp; \text{variance of a sum of IID random variables is sum of their variance} \cr
    &amp;= \frac{1}{n^2} n*\sigma^2 \cr
    &amp;= \frac{\sigma^2}{n} \cr
    \sigma_{\overline{Y}} &amp;= \frac{\sigma}{\sqrt{n}}
    \end{align*}\]</div>
<p>By the way, you might recall our earlier discussion of the total survey error. We said that the following equation held:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \textit{mean squared error} &amp;= \
    \underbrace{\mathbb{E}[(\overline{Y} - \mu_{\overline{Y}})^2]}_{\text{sampling 
    variance}} + \underbrace{(\mu_{\overline{Y}} - \mu_Y)^2}_{\text{bias}}
\end{align*}\]</div>
<p>Note that if we have no bias, and thus <span class="math notranslate nohighlight">\(\mu_{\overline{Y}} = \mu_Y\)</span> (the mean of all the samples is the mean of the population), our sampling variance is the only source of variation. We’ve now derived the sampling variance in more detail.</p>
<p>Finally, here comes our last step. Note that we are planning to do inference on the mean, but we do not know it! How can we use our formula? Well, we can do one of two things.</p>
<p>First, we can simply recognize that, for example, about 68 percent of sample means will fall within +/- two standard deviations of the true population mean. So, we can simply add and subtract one standard error to our sample mean to have a 68 percent chance of having that mean <span class="math notranslate nohighlight">\(\mu_Y\)</span> in there. It’s more conventional to use a higher level of “confidence” and add and subtract 1.96 standard errors to our sample mean to have a 95 percent chance of including <span class="math notranslate nohighlight">\(\mu_Y\)</span>. The general formula for a confidence interval with confidence level <em>C</em> is <span class="math notranslate nohighlight">\(CI_{C} = \overline{y} +/- z_C*\frac{\sigma}{\sqrt{n}}\)</span>, where <span class="math notranslate nohighlight">\(z_C\)</span> is the number of standard deviations within which <em>C</em> percent of the AUC lie. For the sake of our class, just use <span class="math notranslate nohighlight">\(z_{95} = +/- 1.96\)</span> or <span class="math notranslate nohighlight">\(z_{99} = +/- 2.56\)</span>.</p>
<p>Below is a picture of why this works. Suppose that we have population level data on mother’s education. Below is what the sampling distribution would look like, with five sample means plotted and their “error bars” attached. Notice that, although no sample mean is identical, all five sample means with their CIs happen<a class="footnote-reference brackets" href="#fn-bin" id="id5" role="doc-noteref"><span class="fn-bracket">[</span>5<span class="fn-bracket">]</span></a> to include the true population mean. We wouldn’t be able to say, if we just had these samples, where exactly the true population mean was, but we would have captured it in our boundaries (and we could say that this should work, in the long run). In fact, in the long run, we should expect this procedure to work exactly 95 percent of the time.</p>
<p><img alt="The logic of a confidence interval" src="_images/z_CI_demo.png" /></p>
<p>Let’s do a little bit of practice.</p>
<p>Suppose that I have a sample in which the mean of the respondents’ ages is 35, and I want to come up with a plausible range of values for the population mean age. Suppose that <em>n</em> is 900 and I happen to know that the population standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is three (3). What are the boundaries of a 95 percent confidence interval? <a class="footnote-reference brackets" href="#ci" id="id6" role="doc-noteref"><span class="fn-bracket">[</span>6<span class="fn-bracket">]</span></a></p>
<p>Our second technique is a statistical <em>test</em>. This works for fundamentally the same reason as a confidence interval, and they are functionally the same for means (for other kinds of statistics, there will be more reason to prefer the CI or the test). We picture the sampling distribution under a null hypothesis (<span class="math notranslate nohighlight">\(H_0\)</span>) that the true mean is equal to some value <span class="math notranslate nohighlight">\(\mu_0\)</span>, and then we calculate the standard score if that were true and finally find the associated probability. If that <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>-value is small (say, less than 0.05), we reject the null hypothesis. The level at which we reject the null, known as <span class="math notranslate nohighlight">\(\alpha\)</span>, is arbitrary, but a conventional cutoff is 0.05.</p>
<p>It is conventional to take the <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>-value not just of the statistic we actually get, but also its additive inverse. This is shown below. In the example below, we happen to <em>know</em> the true population parameter; this is meant to show you what happens when the null hypothesis is true (sample means are quite rarely more than <span class="math notranslate nohighlight">\(+/- 2*\frac{\sigma}{\sqrt{n}}\)</span> away from the mean.</p>
<p><img alt="The logic of a statistical test" src="_images/ztest_demo.png" /></p>
<p>Let’s again do a little bit of practice.</p>
<p>Go back to the example before in which I have a sample in which the mean of the respondents’ ages is 35, and I want to test the claim that <span class="math notranslate nohighlight">\(\mu_{age} = 35.5\)</span>. How should I do this? First, I calculate my raw difference between the observed mean (35) and the hypothetical mean, <span class="math notranslate nohighlight">\(\mu_0\)</span> = 35.5, which is -0.5. Then, I calculate the standard error, <span class="math notranslate nohighlight">\(\frac{3}{\sqrt{900}}\)</span> = 0.1. Then, I find the test statistic by dividing -0.5 by 0.1 = -5. Finally, I look up the <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>-value for this statistic <em>and</em> the additive inverse (5); I’ll show how to do this with code in class. This is so small that it would not be visible on a picture of the sampling distribution: <span class="math notranslate nohighlight">\(\mathbb{P}(|Z| &gt; |z|) = 0.0000005733\)</span>. Since this sample would be extremely improbable under the null hypothesis, we reject it.</p>
<p>Finally, there is just one more complication to be aware of. In practice, we do not know <span class="math notranslate nohighlight">\(\sigma\)</span>, so we estimate it with <em>s</em>. This causes additional uncertainty in our estimate, and it is now taken from a distribution that is close to Normal but is farther from Normal the smaller the sample size. Many statistical textbooks get very excited about this; I think that this is, for the non-mathematician, a boring technical detail. I will simply say that this means that your should look up your values for the test and the CI in a <em>t</em>-table, not a <em>z</em>-table, but the basic meaning of a CI or <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>-value is the same. If you’re in my SOC357 class, I’ll show how to do this in Google Sheets during our lab.</p>
</section>
</section>
<section id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this heading">#</a></h2>
<section id="an-extremely-useful-alternative-expression-for-the-variance">
<h3>An extremely useful alternative expression for the variance.<a class="headerlink" href="#an-extremely-useful-alternative-expression-for-the-variance" title="Permalink to this heading">#</a></h3>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
            &amp;1. \mathbb{V}[Y] = \mathbb{E}[(Y - \mu_Y)^2] &amp;&amp; \text{definition} \cr
            &amp;2. = \mathbb{E}[Y^2 - 2*\mu_Y*Y + \mu_Y^2] &amp;&amp; \text{binomial expansion AKA &quot;FOILing&quot;} \cr
            &amp;3. = \mathbb{E}[Y^2] - \mathbb{E}[2*\mu_Y*Y] + \mathbb{E}[\mu_Y^2] 
                &amp;&amp; \text{expectation operator is linear} \cr
            &amp;4. = \mathbb{E}[Y^2] - 2*\mu_Y*\mathbb{E}[Y] + \mathbb{E}[\mu_Y^2]
                &amp;&amp; \text{constants can be factored out} \cr
            &amp;5. = \mathbb{E}[Y^2] - 2*\mu_Y*\mu_Y + \mathbb{E}[\mu_Y^2]
                &amp;&amp; \text{definition of expectation} \cr
            &amp;6. = \mathbb{E}[Y^2] - 2*\mu_Y^2 + \mu_Y^2
                &amp;&amp; \text{expectation of a constant is just that constant, definition of square} \cr
            &amp;7. = \mathbb{E}[Y^2] - \mu_Y^2
                &amp;&amp; \text{arithmetic} \cr
            &amp;8. \mathbb{V}[Y] = \mathbb{E}[Y^2] - \mathbb{E}[Y]^2
                &amp;&amp; \text{definition of expectation} \cr
\end{align*}\]</div>
<p>In the sample, this becomes…</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
            &amp;1. s^2 = \frac{\sum_{i=1}^n (y_i - \bar{y})^2}{n-1} &amp;&amp; \text{definition} \cr  
            &amp;2. = \frac{\sum_{i=1}^n (y_i^2 - 2y_i\bar{y} + \bar{y}^2)}{n-1} \
                &amp;&amp; \text{binomial expansion AKA &quot;FOILing&quot;} \cr
            &amp;3. = \frac{\sum_{i=1}^n y_i^2 - \sum_{i=1}^n 2y_i\bar{y} + \sum_{i=1}^n \bar{y}^2}{n-1} 
                &amp;&amp; \text{summation operator is linear} \cr
            &amp;4. = \frac{\sum_{i=1}^n y_i^2 - 2\bar{y}\sum_{i=1}^n y_i + n\bar{y}^2}{n-1} 
                &amp;&amp; \text{constants can be factored out} \cr
            &amp;5. = \frac{\sum_{i=1}^n y_i^2 - 2\bar{y}n\bar{y} + n\bar{y}^2}{n-1} 
                &amp;&amp; \text{definition of summation} \cr
            &amp;6. = \frac{\sum_{i=1}^n y_i^2 - n\bar{y}^2}{n-1} 
                &amp;&amp; \text{arithmetic} \cr
            &amp;7. = \frac{n}{n-1} (\overline{y^2} - \bar{y})^2
                &amp;&amp; \text{arithmetic} \cr
\end{align*}\]</div>
</section>
<section id="the-general-formula-for-the-variance-of-a-sum-of-random-variables">
<h3>The general formula for the variance of a sum of random variables.<a class="headerlink" href="#the-general-formula-for-the-variance-of-a-sum-of-random-variables" title="Permalink to this heading">#</a></h3>
<p>Let <em>Y</em> be the sum of random variables <span class="math notranslate nohighlight">\(Y_1, Y_2, ... Y_k\)</span>. Assume centered variables for the sake of step 5, where we will want to replace each variable’s deviation from its mean with a single symbol to make the multinomial expansion easier. This can be, in principle, any symbol; it seems most natural to just use the variable itself, and we often assume centered variables anyways. Note that there is absolutely no difference if I had used, say, <span class="math notranslate nohighlight">\(\Delta_{Y_j}\)</span> to represent the deviation of the <em>j</em>th variable from its mean.</p>
<p>To get an intuition for the correct algorithm for multinomial expansion, draw a picture of rectangle with both unique side lengths partitioned into <span class="math notranslate nohighlight">\(Y_1, Y_2, ... Y_k\)</span>. Then, find the area of the rectangle, which is logically equal, of course, to finding <span class="math notranslate nohighlight">\((Y_1 + Y_2 + ... Y_k)^2\)</span>. The rectangle is now comprised of smaller rectangles with areas <span class="math notranslate nohighlight">\((Y_1*Y_1), (Y_2*Y_1) ... (Y_k*Y_1)\)</span> going down the first column, <span class="math notranslate nohighlight">\((Y_1*Y_2), (Y_2*Y_2) ... (Y_k*Y_2)\)</span> going down the second, etc. We can thus visualize this as operation as follows: write out <span class="math notranslate nohighlight">\((Y_1 + Y_2 + ... Y_k)^2\)</span> as <span class="math notranslate nohighlight">\((Y_1 + Y_2 + ... Y_k)*(Y_1 + Y_2 + ... Y_k)\)</span>. Starting with the first (or second; it doesn’t matter) set of parentheses, take each term, multiply it by every term in the second set of parentheses, then add them up, and then do this for each term in the first set. Then, add up all the summed terms. This corresponds to summing up all of the items in the variance-covariance matrix. It was too time-consuming for me to draw a picture of all of this in Markdown, but DeVellis (2003) is a good, unintimidating visual representation of this sort of proof (which is really useful in general in the context of statistics).</p>
<p>By the way, it may be useful to note that to sum all entries in a matrix, we can write it as a quadratic form. If we have centered variables, our covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is simply <span class="math notranslate nohighlight">\(\frac{1}{n-1}\textbf{X}^t\textbf{X}\)</span>, where <span class="math notranslate nohighlight">\(\textbf{X}\)</span> is the data matrix. Then, to sum all elements in that matrix, we write. <span class="math notranslate nohighlight">\(\vec{1}^t\boldsymbol{\Sigma}\vec{1}\)</span>. This proof won’t use matrix properties, though.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
            &amp;1. \mathbb{V}[Y] = \mathbb{E}[(Y - \mu_Y)^2] &amp;&amp; \text{definition of variance} \cr
            &amp;2. = \mathbb{E}[([Y_1 + Y_2 + ... Y_k] - \mu_Y)^2] &amp;&amp; \text{definition of variable Y} \cr
            &amp;3. = \mathbb{E}[([Y_1 + Y_2 + ... Y_k] - [\mu_{Y_1} + \mu_{Y_2} + ... \mu_{Y_k}])^2] \
                &amp;&amp; \text{expectation operator is linear} \cr
            &amp;4. = \mathbb{E}[(Y_1 - \mu_{Y_1} + Y_2 - \mu_{Y_2} + ... Y_k - \mu_{Y_k})^2]
                &amp;&amp; \text{commutativity of addition} \cr
            &amp;5. = \mathbb{E}[(Y_1 + Y_2 + ... Y_k)^2]
                &amp;&amp; \text{definition of centered variables} \cr
            &amp;6. = \mathbb{E}[Y_1^2 + (Y_2 * Y_1) + ... (Y_k * Y_1) + (Y_1 * Y_2) + Y_2^2 + (Y_3 * Y_2) ... ]
                &amp;&amp; \text{picture the rectangle as mentioned above} \cr
            &amp;7. = \mathbb{E}[\sum_{j=1}^k\sum_{i=1}^k Y_i * Y_j]
                &amp;&amp; \text{generalizing the pattern} \cr
            &amp;8. = \sum_{j=1}^k\sum_{i=1}^k \sigma^2_{Y_i, Y_j}
                &amp;&amp; \text{definition of(co)variances} \cr
            &amp;9. = \sum_{j=1}^k \sigma^2_{Y_j} + 2\sum_{i&gt;j}^k\sum_{j=1}^k \sigma^2_{Y_i, Y_j}
                &amp;&amp; \text{covariance matrix is symmetric}
        \end{align*}\]</div>
<p>Of course, we often work with variables—such as the sample means of women’s education and that of men, <span class="math notranslate nohighlight">\(\overline{Y}_F\)</span> and <span class="math notranslate nohighlight">\(\overline{Y}_{M}\)</span>—whose population covariance <span class="math notranslate nohighlight">\(\sigma_{\overline{Y}_F, \overline{Y}_M}\)</span> is equal to zero. Then, only the first term of line 9 above is relevant to the calculation of the variance of a random variable which is itself the summation or addition of other random variables.</p>
</section>
<section id="the-surprising-demonstration-of-any-element-s-probability-of-inclusion-when-sampling-from-a-finite-population-without-replacement">
<h3>The surprising demonstration of any element’s probability of inclusion when sampling from a finite population without replacement<a class="headerlink" href="#the-surprising-demonstration-of-any-element-s-probability-of-inclusion-when-sampling-from-a-finite-population-without-replacement" title="Permalink to this heading">#</a></h3>
<p>It turns out that any element has probability <span class="math notranslate nohighlight">\(\frac{n}{N}\)</span> of being selected into the sample from a finite population. This is obvious when sampling with replacement. We have <em>n</em> mutually exclusive or disjoint events, the selection of <em>n</em> elements into our sample. Each time, we have probability <span class="math notranslate nohighlight">\(\frac{1}{N}\)</span>, so we simply add up the probabilities <em>n</em> times.</p>
<p>This is also the correct answer for a sample <em>without</em> replacement, but this is, in some respects a coincidence since the probability of inclusion <em>appears</em> to change each time since the population does (it loses a member). Here is the proof (this is the developed form a proof telegraphed in the classic Kish [1965]).</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;1. \text{number of ways to pick any given sample}) = {N \choose n} \
    &amp;&amp; \text{definition of binomial coefficient} \cr
&amp;2. \text{num. ways to select sample which definitely has element} j = {N-1 \choose n-1} \
    &amp;&amp; \text{if we must include $j$, population and sample decline by one} \cr
&amp;3. \mathbb{P}(\text{select element} j) = \frac{\text{step 2}}{\text{step 1}} \
    &amp;&amp; \text{probability =} \frac{n_{\text{ways to succed}}}{n_{\text{outcomes}}} \cr
&amp;4. \mathbb{P}(\text{select element} j) = \frac{{N-1 \choose n-1}}{{N \choose n}} \
    &amp;&amp; \text{substitution} \cr
&amp;5. = \frac{\frac{(N-1)!}{(n-1)!(N-1 - n-1)!}}{\frac{N!}{n! (N-n)!}} \
    &amp;&amp; \text{definition} \cr
&amp;6. = \frac{\frac{(N-1)!}{(n-1)! (N-n)!}}{\frac{N!}{n!(N-n)!}} \
    &amp;&amp; \text{simplification} \cr
&amp;7. = \frac{\frac{(N-1)!}{(n-1)!}}{\frac{N!}{n!}} \
    &amp;&amp; \text{more simplification} \cr
&amp;8. = \frac{(N-1)!n!}{N!(n-1)!} \
    &amp;&amp; \text{more simplification} \cr
&amp;8. = \frac{n}{N} \
    &amp;&amp; \text{definition of factorials} \cr
\end{align*}\]</div>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="answer" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>3 + 10 + 2 + 4.5 = 19.5</p>
</aside>
<aside class="footnote brackets" id="answer3" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>(3 + 10 + 2 + 4.5)/4 = 4.875</p>
</aside>
<aside class="footnote brackets" id="answer4" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Individual 1’s squared deviation is (3-4.875)^2 = 3.52. Individual 2’s squared deviation is
(10-4.875)^2 = 26.27. Individual 3’s squared deviation is (2-4.875)^2 = 8.27. Individual 4’s squared
deviation is (4.5-4.875)^2 = 0.14. The total sum of squares (TSS) is 38.2; the (near) mean is 38.2/3 =
12.73 and the root thereof is 3.57.</p>
</aside>
<aside class="footnote brackets" id="fn-auc" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Formally, where <span class="math notranslate nohighlight">\(f(X)\)</span> is the generic notation for a PDF, <span class="math notranslate nohighlight">\(\int_a^b f(X) = \mathbb{P}(a \leq X \leq b)\)</span>, i.e. the PDF <span class="math notranslate nohighlight">\(f(x)\)</span> is the derivative of the CDF <span class="math notranslate nohighlight">\(F(X)\)</span>, so integrating <span class="math notranslate nohighlight">\(f(x)\)</span> from <em>a</em> to <em>b</em> gives the change in height on the CDF from <em>a</em> to <em>b</em> (which is, more obviously, the the probability that a variable takes a value between <em>a</em> and <em>b</em>).</p>
</aside>
<aside class="footnote brackets" id="fn-bin" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Incidentally, the probability of five randomly drawn sample means, each with <span class="math notranslate nohighlight">\(\mathbb{P}\)</span>(CI contains <span class="math notranslate nohighlight">\(\mu_Y\)</span>) = 0.95, all including <span class="math notranslate nohighlight">\(\mu_Y\)</span> is, from the binomial distribution, <span class="math notranslate nohighlight">\(\binom{5}{5}*0.95^5*0.5^{5-5} = 1 * 0.95^5 * 1 = 0.77\)</span>, or about 77 percent.</p>
</aside>
<aside class="footnote brackets" id="ci" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">6</a><span class="fn-bracket">]</span></span>
<p>95 CI = <span class="math notranslate nohighlight">\(\overline{y} +/- z_{95}*SE = 35 +/- 1.96*\frac{3}{\sqrt{900}} = 35 +/- 0.196 = (34.80, 35.19)\)</span></p>
</aside>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="2023-06-16_SOC357su23_syllabus.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Course parameters</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#some-basic-operations">Some basic operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distributions-regular-variable-and-sampling">Distributions: regular variable and sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discrete-distributions-cumulative-distribution-and-probability-mass-functions">Discrete distributions: cumulative distribution and probability mass functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-distributions-probability-density-functions">Continuous distributions: probability density functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basics-of-inference">Basics of inference</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix">Appendix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#an-extremely-useful-alternative-expression-for-the-variance">An extremely useful alternative expression for the variance.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-general-formula-for-the-variance-of-a-sum-of-random-variables">The general formula for the variance of a sum of random variables.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-surprising-demonstration-of-any-element-s-probability-of-inclusion-when-sampling-from-a-finite-population-without-replacement">The surprising demonstration of any element’s probability of inclusion when sampling from a finite population without replacement</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By gjmb
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>