{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66667ff8",
   "metadata": {},
   "source": [
    "# basic math for social sciences I: basic descriptive and inferential statistics\n",
    "\n",
    "\n",
    "## Some basic operators\n",
    "\n",
    "1. **The summation operator, $\\sum$**, tells us to sum from the observation denoted at the bottom of the capital greek letter $\\Sigma$ (\"sigma\", which makes the \"s\" sound, as in **s**um) and go up until the observation denoted on top of the $\\Sigma$. For us, the bottom item will almost always be individual (denoted *i*) equal to one, going up until individual equal to *n*, where *n* is the sample size. Please note carefully that we're using small Latin letters here in two different ways: *i* is an index, which varies across individuals; *n* is a constant and a property of the data-set as a whole. Examine this simple example data-set: \n",
    "\n",
    "    $$\n",
    "    \\begin{array}{|c|c|}\n",
    "        \\hline\n",
    "            \\textbf{i} & \\textbf{y} \\\\\n",
    "        \\hline\n",
    "            \\text{1} & \\text{3} \\\\\n",
    "        \\hline\n",
    "            \\text{2} & \\text{10} \\\\\n",
    "        \\hline\n",
    "            \\text{3} & \\text{2} \\\\\n",
    "        \\hline\n",
    "            \\text{4} & \\text{4.5} \\\\\n",
    "        \\hline\n",
    "    \\end{array}\n",
    "    $$\n",
    "\n",
    "    Now, try to carry out the arithmetic operation indicated by the following notation: $\\sum_{i=1}^n \n",
    "    y_i$.[^answer]\n",
    "    \n",
    "    [^answer]: 3 + 10 + 2 + 4.5 = 19.5\n",
    "\n",
    "    The numbering of individuals is, for our purposes, basically always arbitrary, and summation is \n",
    "    commutative anyways (the order of the items summed, or the *summands*, does not matter), so sometimes you \n",
    "    will see the sub- and superscripts omitted. \n",
    "\n",
    "    Let's suppose that we are interested in someone's highest year of education achieved and we have \n",
    "    observations on *n* = 10 individuals. Let their scores on the education variable be represented by the \n",
    "    vector $\\vec{y} = [10, 12, 16, 12, 18, 10, 20, 18, 9, 10]^T$. All that the little \"T\" represents here is \n",
    "    that this is the transpose of the actual vector, which would be a column in the standard way of \n",
    "    representing data in matrices (rows are individuals, columns are variables). As you can see above, it \n",
    "    wastes a lot of space to represent column vectors the correct way in text documents, so you will see \n",
    "    this convention very often. The idea is that we are making clear that this is a list of different \n",
    "    individuals and *not* the scores for one individual on ten different variables (if we wanted to represent \n",
    "    such a row vector, we'd wouldn't need a $^T$. \n",
    "    \n",
    "    Now, try to sum up these individuals' scores. The order here is arbitrary, but you might as well \n",
    "    conceptualize those scores above as being a specific case of the following \n",
    "    vector: $\\vec{y} = [y_1, y_2, ... y_n]^T$. The last piece of notation is that we we will represent random \n",
    "    variables in their most\n",
    "    abstract form with capital Roman letters; their sample equivalents have lowercase Roman letters. Unlike\n",
    "    in most statistics books, I will consistently use Greek letters without hats to denote true population \n",
    "    parameters (most books usually, but not always, do this) and Greek letters with carets or \"hats\". Greek\n",
    "    letters will be used in the conventional way: the sound they make indicates the parameter. Here, $\\tau$\n",
    "    (\"tau\") stands in for the total. Note that the notation means that to find the sample estimate of the \n",
    "    total of the random variable *Y*, we sum up the observed values in the sample, $y_i$. Note that to \n",
    "    estimate the value of the total, we need to multiply by the probability of selection into the sample. If \n",
    "    each observation has a different probability, we need to put this inside of the summation. In our case, \n",
    "    we'll deal only with the equal probability of selection (*epsem*) method, so this is a constant that can \n",
    "    be factored out. \n",
    "    \n",
    "    \\begin{align*}\n",
    "        \\widehat{\\tau}_Y &= \\pi \\sum_{i=1}^n y_i\n",
    "    \\end{align*}\n",
    "\n",
    "2. **The mean** is a measure of central tendency&mdash;something like the \"characteristic value of a distribution\", which the mean, median and mode all get at. We focus most attention on the mean in this class; however, the median is an important measure, especially in the context of quantiles more generally. **\"Mean\" is simply the formal term for the arithmetic average.** We'll focus on the simple case where possible outcomes are discrete, meaning that they are finite in number or **countably** infinite (more in a second); simple descriptive statistics for any real world data-set can be calculated using these simpler discrete formulae. Sometimes, we will need to discuss variables with infinite possible values, known as continuous variables. \n",
    "\n",
    "    For example, if we record someone's income to the cent, while they could in theory tell us that they have \n",
    "    any number up to infinity, this is *countably* infinite; setting aside the formal definition, this means \n",
    "    basically that if I pick an arbitrary number of functioning cars someone owns (say, five cars), I can tell \n",
    "    you the next value in the set of possible outcomes with no question (six cars); so, cars owned is a \n",
    "    discrete variable. \n",
    "    \n",
    "    By contrast, if we ask someone about their \n",
    "    height and they reply with \"74 inches\", I can't say what height is \"next\". I could, for example, say 75 \n",
    "    inches, but since 74 inches is about 188cm, 189cm is nearer than 75 inches. And this, of course, can \n",
    "    extend even further, to the micron (if we had a microscope). Since height can take on an uncountable \n",
    "    number of possible values: it is continuous. (It is fun, but not super important, to wonder about why some \n",
    "    variables are continuous and others are discrete. In the cars case, it is just definitional; while parts \n",
    "    of cars do exist, if I am only interested in complete, functioning cars, those simply do not come in \n",
    "    halves; by contrast, there is no similar limit on height).\n",
    "    \n",
    "    **The mean is sometimes referred to as the expected value or expectation.** When we write the mean as an \n",
    "    expected value or expectation, we conceptualize it as a population-level property; it is the weighted sum \n",
    "    of all possible values of the variable. So, we are summing over the possible outcomes of the variable when \n",
    "    we write the mean as an expectation&mdash;we are **not** summing over any observed individuals. \n",
    "    Many textbooks and internet sources do not change their notation here, which confuses new students, so I \n",
    "    will now re-index the sum. Let *k* index an outcome of the random variable *Y* (i.e., \"*k*=5\" would mean \n",
    "    the fifth possible outcome, where order again typically does not matter) and let *K* indicate the number \n",
    "    of possible outcomes. Then, for a *discrete* random variable (don't worry about the continuous formlua \n",
    "    here):  \n",
    "    \n",
    "    \\begin{align*}\n",
    "    \\mathbb{E}[Y] &= \\sum_{k=1}^K y_k * \\mathbb{P}(Y=y_k)\n",
    "    \\end{align*}\n",
    "\n",
    "    This says that we look at each possible outcome *k*, starting with *k* = 1, and multiple the value of that \n",
    "    outcome $y_k$ by its probability of occurring, $\\mathbb{P}(Y=y_k)$. \n",
    "    \n",
    "    For a given *set* of data, the formula does not change, even if the variable is itself continuous; this is \n",
    "    because all actual observed data-sets are discrete: there is a finite number of observed values. If we \n",
    "    have any repeated values, we effectively use this as a sample estimate of the probability of observing \n",
    "    that value at  the population level. If the variable itself has many possible outcomes (whether it is \n",
    "    discrete or continuous) and no values repeat themselves exactly, then the estimated probability of each \n",
    "    value is just $\\frac{1}{n}$. \n",
    "\n",
    "    So, in practice, with equal probability sampling, it is often simpler to write out the mean as the simple \n",
    "    arithmetic mean, even if we have repeat values. \n",
    "   \n",
    "    Note one exception for our notation rules mentioned above: It would make *most* sense to use, for the \n",
    "    sample mean, the notation $\\widehat{\\mu}_Y$. However, most textbooks use the notation $\\bar{Y}$ for the \n",
    "    random variable and $\\bar{y}$ for some specific sample mean. The problem with this notation is that it \n",
    "    breaks the general rule about Greek and Roman letters mentioned above and requires you to  memorize a new \n",
    "    notational rule, but it is extremely common, so we will use it. For any specific sample mean...\n",
    "    \n",
    "    \\begin{align*}\n",
    "    \\bar{y} &= \\sum_{i=1}^n \\frac{1}{n} * y_i \\cr\n",
    "    \\bar{y} &=  \\frac{1}{n} \\sum_{i=1}^n* y_i  \\\n",
    "        &&  \\text{since $\\frac{1}{n}$ is constant, it factors out of the sum} \\cr\n",
    "    \\end{align*}\n",
    "    \n",
    "    Note that this means that a *simple* mean implicitly takes the inclusion of each member of the sample to \n",
    "    have been equally probable. This is false with a stratified random sample. So, we fix this by multiplying \n",
    "    by the inverse of the probability of inclusion, the *sample weight* $w = \\frac{1}{\\pi_i}$, where $\\pi_i$ \n",
    "    is *i*'s probability of being included in the sample as $\\pi_i$. Note that $\\pi_i$ is a parameter, not a \n",
    "    statistic; every member of our sample has a *known* probability of inclusion. \n",
    "    In the case of simple random sampling, every member of the population has an equal probability of \n",
    "    inclusion of $\\frac{n}{N}$, where *n* is the sample size and *N* is the population size. \n",
    "    \n",
    "    \\begin{align*}\n",
    "     \\bar{y} &= \\frac{1}{N} \\sum_{i=1}^n \\frac{1}{\\pi_i} * y_i \\cr\n",
    "    &= \\frac{1}{N} \\sum_{i=1}^n \\frac{N}{n} * y_i && \\text{note that $\\pi_i$ = $\\frac{n}{N}$} \\cr\n",
    "    &= \\frac{1}{N} * \\frac{N}{n} \\sum_{i=1}^n y_i && \\text{constants factor out of sums} \\cr\n",
    "    &= \\frac{1}{n}\\sum_{i=1}^n y_i\n",
    "    \\end{align*}\n",
    "    \n",
    "    Note that this formula is helpful because is can be extended to cases where we don't have equal \n",
    "    probabilities of inclusion. \n",
    "    \n",
    "    For some practice, try calculating the sample mean for the data-set given above.[^answer3]\n",
    "    \n",
    "    [^answer3]: (3 + 10 + 2 + 4.5)/4 = 4.875\n",
    "\n",
    "3. **The variance** is a measure of spread or dispersion. You can think about it as the expected difference between a randomly-selected observation and its mean. We define it formally at the population-level as the expectation of the difference between the random variable *Y* and its mean $\\mu_Y: \\mathbb{V}[Y] = \\mathbb{E}[(Y - \\mu)^2]$. You may wonder why we square the difference. There is a very good reason for this that leads us to the proof of a useful fact about the mean, which is that it is the value for a given set about which all deviations are not just minimized but also zero!\n",
    "\n",
    "    \\begin{align*}\n",
    "    &1. \\mathbb{E}[Y - \\mu_Y] = \\mathbb{E}[Y] - \\mathbb{E}[\\mu_Y] \n",
    "                && \\text{expectation operator is linear} \\cr\n",
    "    &2. \\mathbb{E}[Y] = \\mu_Y && \\text{by definition} \\cr\n",
    "    &3. \\mathbb{E}[\\mu_Y] = \\mu_Y && \\text{expectation of a constant is just that constant} \\cr\n",
    "    &4. \\mathbb{E}[Y - \\mu_Y] = \\mu_Y - \\mu_Y = 0  && \\text{arithmetic}\n",
    "        \\end{align*}   \n",
    "        \n",
    "    It is useful to think about the variance operator, $\\mathbb{V}$ as a function that produces the variance, and the actual variance itself as a fixed quantity for a given population. We will denote this $\\sigma^2$. Do not let yourself get frustrated by the power of 2. The reason we do this is quite simple: the variance is easier to *calculate* with, but the interpretation&mdash;the expected squared distance of a point from the mean, or the average squared distance of a point from the mean&mdash;is more difficult. One solution is to just take the root of the whole thing, which we then call the standard deviation, which is $\\sigma$ for the population and *s* in the sample (the sample variance is then *s*^2). \n",
    "    \n",
    "    The sample variance formula is also straightforward, with the exception of the denominator. Don't worry too much about this; it is called the Bessel correction, and it fixes the bias caused by the fact that we estimate the population mean with the sample mean, which understands the true variance (the spread of the points will generally be closer to the sample mean than the population mean). But, the variance is still fundamentally the mean squared deviation. \n",
    "    \n",
    "    \\begin{align*}\n",
    "        s^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n-1} \\cr\n",
    "        s = \\sqrt{\\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n-1}}\n",
    "    \\end{align*}\n",
    "\n",
    "    Finally, try to calculate the sample variance of our data-set above. [^answer4]\n",
    "    \n",
    "    [^answer4]: Individual 1's squared deviation is (3-4.875)^2 = 3.52. Individual 2's squared deviation is \n",
    "    (10-4.875)^2 = 26.27. Individual 3's squared deviation is (2-4.875)^2 = 8.27. Individual 4's squared \n",
    "    deviation is (4.5-4.875)^2 = 0.14. The total sum of squares (TSS) is 38.2; the (near) mean is 38.2/3 = \n",
    "    12.73 and the root thereof is 3.57. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55aa806",
   "metadata": {},
   "source": [
    "# Distributions: regular variable and sampling\n",
    "\n",
    "## Discrete distributions: cumulative distribution and probability mass functions \n",
    "\n",
    "We saw above some methods of characterizing distributions by their parameters (their mean and standard deviation), but what if we want to examine the distribution overall? We need to look at this separately for discrete and continuous variables. We'll start with what I'm calling *regular variable* distributions, distributions that characterize an actual variable of interest, such as mother's education or income or age; then, we'll turn to *sampling* distributions, distributions that characterize the behavior of a sample statistic over many different samples. This second quantity is of interest to us for *inferential reasons*; we care about income, but to make inference about income, we need to understand the behavior of sample means of income across many samples. This difference is *extremely* important, and in many years of teaching this material, I have found that the most common mistake students make is to ignore this distinction or to realize that one does not understand it but to be excessively easygoing about this fact. You *must* make sure that you understand this difference. \n",
    "\n",
    "There are two useful ways to examine an individual distribution for discrete variables. The first is a cumulative distribution function (CDF), which plots on the X-axis the value of the variable and on the Y-axis, the cumulative percent (i.e., probability) of data observed up to that value. A CDF makes sense for both continuous and discrete random variables. The height of this function is the percent of observations less than or equal to some value of the variable; remembering that percent and probabilities usually are the same thing, and letting $\\mathbb{P}$ mean \"probability\", the height of this function $F(X) = \\mathbb{P}(X \\leq x)$. \n",
    "\n",
    "The second is a probability mass function (PMF), which plots possible values of the variable on the X-axis (again), but this time plots the probability of observations *just for that value* on the Y-axis. You can see these two approaches belows. Notice that a PMF is basically just a histogram. The height of this function is the percent of observations *exactly* equal to some value of the variable; the height of this function $F(X) = \\mathbb{P}(X = x)$. \n",
    "\n",
    "![CDF and PMF](./figures/pmfcdfnumkids.png)\n",
    "\n",
    "## Continuous distributions: probability density functions \n",
    "\n",
    "For continuous variables, we can also generate cumulative distribution functions, but probability mass functions don't, strictly speaking, make sense. This is because the probability of any exact value of such a variable is zero. This is surprising at first, but if you have trouble buying this, imagine the probability of any specific value of a variable which could possibly be equal to, say, $\\pi$ or *e* (i.e., irrational numbers)&mdash;if it's not zero, it's extremely small, right? \n",
    "\n",
    "It is important to note that *all* actual, observed populations are, practically speaking, \"discrete\" in that the number of outcomes is finite. So, while a variable can have a PDF, any dataset of *realizations* cannot; we just have an estimate of the PDF, which is actually a PMF. For example, here is the empirical CDF and PMF of height from the 2017-18 NHANES. Height is a continuous variable, but the realization of it in this sample is discrete, even if it is massively less-discrete than the number of children seen above. If you look carefully, you can see the slight \"kinks\" in the line; these are not problems of pixellation but rather that this is an empirical estimate of continuous function. \n",
    "\n",
    "![CDF and PMF for realization of continuous variable](./figures/height_CDF_P'D'F.png)\n",
    "\n",
    "What do actual density functions look like and represent, then? Cumulative distribution functions look nearly the same, and the function F(X) means the same thing: the probability that the variable takes on value less than or equal to some value X; formally $\\mathbb{P}(X \\leq x)$. *Density* is a trickier concept; what it means formally is *probability per unit*. You should think about density as the \"speed of probability\"; the PDF is just a graph of the *speed* of the CDF (formally, the PDF is the simple first derivative of the CDF). So, note that the graph of the PDF is *tallest* where the slope of the CDF is *steepest*. \n",
    "\n",
    "Importantly, this means that the *height* of the PDF is not of direct interest, both because the height is hard to interpret intuitively, even if the mathematical definition is simple; and, at any rate, . What we care about instead are *areas under the curve* between two points *a* and *b*, which *are* probabilities (not densities) that the variable will take on a value between *a* and *b*.[^fn_auc]\n",
    "\n",
    "[^fn_auc]: Formally, where $f(X)$ is the generic notation for a PDF, $\\int_a^b f(X) = \\mathbb{P}(a \\leq X \\leq b)$.\n",
    "\n",
    "So, what are PDFs good for? Well, we can use them to model the probability distribution of a random variable. Remarkably, it turns out that the random variable $\\overline{Y}$ is 1) continuous and 2) Normally-distributed, even if the distribution of the random variable *Y* is far from Normally-distributed&mdash;this is the power of the Central Limit Theorem. Here is a simulation we've seen before in this class, using a large number of replications (so, this is not the true PDF, but it gives us a clear sense of what that would look like). Further, we happen to know its \n",
    "\n",
    "![The power of the central limit theorem](./figures/CLT_power.png)\n",
    "\n",
    "Here are the Standard Normal CDF and PDF, which characterize the distribution of standardized sample means. All sample means are Normally-distributed, but they have arbitrary means and standard deviations; subtracting the mean and dividing by the standard deviation gives a standard score $z = \\frac{\\overline{y} - \\mu_{\\overline{Y}}}{\\sigma_{\\overline{Y}}}$. We typically won't actually know the mean and standard deviation, but I'll address that in a little bit. \n",
    "\n",
    "![Standard Normal CDF and PDF](./figures/Normal_CDF_PDF.png)\n",
    "\n",
    "Now, it is worth noting that although it is difficult to take the areas under the curve (AUC) of the Normal by hand with calculus, it is easy to do so with a computer or a table. Further, the AUCs associated with +/- one, two, and three standard deviations are easy to memorize. In particular, the fact that going out exactly 1.96 standard deviations includes almost exactly 95 percent AUC is very easy to remember, and if you've heard of a 95 percent confidence interval (which is extremely standard, though technically arbitrary), this is why.\n",
    "\n",
    "![Areas under the curve of the NormalPDF](./figures/Normal_PDF_AUCs.png)\n",
    "\n",
    "If the population is sufficiently large relative to the the size of the sample, each individual observation can be treated as independent of the others and taken from the same distribution (so long as they are drawn at random): they are \"independent, identically-distributed\" (IID) random variables.\n",
    "\n",
    "This concept is often very challenging for novices, so I want you to go back and re-read the preceding paragraph. *Each person in our sample, before they are actually selected, is a random variable*. For example, if I plan to sample 500 people and ask them their income, but I have not yet done so, those 500 hypothetical-people represent 500 random variables, all taken from the same population distribution of income. It is often helpful to consider the analogy to an experiment in which we flip coins: the concept of \"the outcome of the 10th coin I flip\" is more-obviously a random variable for the simple reason that it hasn't yet happened. While the person whose income I ultimately observe in the sample has already *obtained* that income, there is a point at which I don't know *which* person I'll observe, and thus that \"person's\" income is rather like the outcome of the as-yet-unflipped coin. \n",
    "\n",
    "Here is the extremely important fact about that. When we do inference about a population, we cannot observe it, and we do not know its distribution; its distribution likely does not fit some exact mathematical function anyways (and it may not be well-approximated by one). So, observing the realization of *one* random variable&mdash;taking one person out of the population and asking, say, their income&mdash;is not very informative. When we turn a set of random variables into a total or a mean, however, we know that the shape of the distribution *they* come from is not the (unknown) \"regular variable\" distribution but the sampling distribution. *This is the primary reason why (pre-super computer) statistics is even possible*. It is such a striking fact that Galton claimed that \"[[t]he law would have been personified by the Greeks and deified, if they had known of it](https://galton.org/cgi-bin/searchImages/galton/search/books/natural-inheritance/pages/natural-inheritance_0073.htm)\". \n",
    "\n",
    "## Basics of inference\n",
    "\n",
    "So, we can now do inference! All we need to do is find the standard score of a sample mean and then find the associated AUC. Recall before that sample means are unbiased estimators for simple random samples, so our mean of the sampling distribution, $\\mu_{\\overline{Y}}$, is simply the population mean, $\\mu_Y$. \n",
    "\n",
    "Now, we are after the standard deviation of the sample mean, which is referred to as the standard error for the sake of distinguishing it from the \"regular variable\" standard deviation. It turns out that it is much easier to derive the variance and then simply take the root. Note that we are after the variance of a sum of IID random variables (divided by the constant *n*).  While the extremely useful general formula for the variance of a sum of random variables is slightly complex (see appendix), it is not just extremely useful but extremely simple for independent random variables: it is just the sum of their variances. Let $\\overline{Y}$ represent the sample mean of a random variable comprised of *n* IID random variables $Y_1, Y_2, ... Y_n$; each variable comes from a population distribution with standard deviation $\\sigma_Y$, which we'll simply refer to using $\\sigma$ for simplicity.  \n",
    "\n",
    "   \\begin{align*}\n",
    "    \\mathbb{V}[\\overline{Y}] = \\sigma^2_{\\overline{Y}} &= \\sum_{i=1}^n \\mathbb{V}[\\frac{Y_i}{n}] \\cr\n",
    "    &= \\sum_{i=1}^n \\frac{1}{n^2} \\mathbb{V}[Y_i] \\cr\n",
    "    &= \\frac{1}{n^2} (\\sigma^2_1 + \\sigma^2_2 + ... + \\sigma^2_n) \\cr\n",
    "    &= \\frac{1}{n^2} n*\\sigma^2 \\cr\n",
    "    &= \\frac{\\sigma^2}{n} \\cr\n",
    "    \\sigma_{\\overline{Y}} &= \\frac{\\sigma}{\\sqrt{n}}\n",
    "    \\end{align*}\n",
    "\n",
    "Finally, here comes our last step. Note that we are planning to do inference on the mean, but we do not know it! How can we use our formula? Well, we can do one of two things. \n",
    "\n",
    "First, we can simply recognize that, for example, about 68 percent of sample means will fall within +/- two standard deviations of the true population mean. So, we can simply add and subtract one standard error to our sample mean to have a 68 percent chance of having that mean $\\mu_Y$ in there. It's more conventional to use a higher level of \"confidence\" and add and subtract 1.96 standard errors to our sample mean to have a 95 percent chance of including $\\mu_Y$. The general formula for a confidence interval with confidence level *C* is $CI_{C} = \\overline{y} +/- z_C*\\frac{\\sigma}{\\sqrt{n}}$, where $z_C$ is the number of standard deviations within which *C* percent of the AUC lie. For the sake of our class, just use $z_{95} = 1.96$. \n",
    "\n",
    "Below is a picture of why this works. Suppose that we have population level data on mother's education. Below is what the sampling distribution would look like, with five sample means plotted and their \"error bars\" attached. Notice that, although no sample mean is identical, all five happen[^fn_bin] to include the true population mean. In the long run, we should expect this procedure to work 95 percent of the time. \n",
    "\n",
    "[^fn_bin]: Incidentally, the probability of five randomly drawn sample means, each with $\\mathbb{P}$(CI contains $\\mu_Y$) = 0.95, all including $\\mu_Y$ is, from the binomial distribution, $\\binom{5}{5}*0.95^5*0.5^{5-5} = 1 * 0.95^5 * 1 = 0.77$, or about 77 percent.\n",
    "\n",
    "![The logic of a confidence interval](./figures/z_CI_demo.png)\n",
    "\n",
    "Our second technique is a statistical *test*. This works for fundamentally the same reason as a confidence interval, and they are functionally the same for means (for other kinds of statistics, there will be more reason to prefer the CI or the test). We picture the sampling distribution under a null hypothesis ($H_0$) that the true mean is equal to some value $\\mu_0$, and then we calculate the standard score if that were true and finally find the associated probability. If that $\\mathbb{P}$-value is small (say, less than 0.05), we reject the null hypothesis. The level at which we reject the null, known as $\\alpha$, is arbitrary. \n",
    "\n",
    "It is conventional to take the $\\mathbb{P}$-value not just of the statistic we actually get, but also its additive inverse. This is shown below. \n",
    "\n",
    "![The logic of a statistical test](./figures/ztest_demo.png)\n",
    "\n",
    "Finally, there is just one more complication to be aware of. In practice, we do not know $\\sigma$, so we estimate it with *s*. This causes additional uncertainty in our estimate, and it is now taken from a distribution that is close to Normal but is farther from Normal the smaller the sample size. Many statistical textbooks get very excited about this; I think that this is, for the non-mathematician, a boring technical detail. I will simply say that this means that your should look up your test statistic in a *t*-table, not a *z*-table, but the basic meaning of a CI or $\\mathbb{P}$-value is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095a821",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### An extremely useful alternative expression for the variance.\n",
    "\n",
    "\\begin{align*}\n",
    "            &1. \\mathbb{V}[Y] = \\mathbb{E}[(Y - \\mu_Y)^2] && \\text{definition} \\cr\n",
    "            &2. = \\mathbb{E}[Y^2 - 2*\\mu_Y*Y + \\mu_Y^2] && \\text{binomial expansion AKA \"FOILing\"} \\cr\n",
    "            &3. = \\mathbb{E}[Y^2] - \\mathbb{E}[2*\\mu_Y*Y] + \\mathbb{E}[\\mu_Y^2] \n",
    "                && \\text{expectation operator is linear} \\cr\n",
    "            &4. = \\mathbb{E}[Y^2] - 2*\\mu_Y*\\mathbb{E}[Y] + \\mathbb{E}[\\mu_Y^2]\n",
    "                && \\text{constants can be factored out} \\cr\n",
    "            &5. = \\mathbb{E}[Y^2] - 2*\\mu_Y*\\mu_Y + \\mathbb{E}[\\mu_Y^2]\n",
    "                && \\text{definition of expectation} \\cr\n",
    "            &6. = \\mathbb{E}[Y^2] - 2*\\mu_Y^2 + \\mu_Y^2\n",
    "                && \\text{expectation of a constant is just that constant, definition of square} \\cr\n",
    "            &7. = \\mathbb{E}[Y^2] - \\mu_Y^2\n",
    "                && \\text{arithmetic} \\cr\n",
    "            &8. \\mathbb{V}[Y] = \\mathbb{E}[Y^2] - \\mathbb{E}[Y]^2\n",
    "                && \\text{definition of expectation} \\cr\n",
    "\\end{align*}\n",
    "        \n",
    "In the sample, this becomes...\n",
    "\n",
    "\\begin{align*}\n",
    "            &1. s^2 = \\frac{\\sum_{i=1}^n (y_i - \\bar{y})^2}{n-1} && \\text{definition} \\cr  \n",
    "            &2. = \\frac{\\sum_{i=1}^n (y_i^2 - 2y_i\\bar{y} + \\bar{y}^2)}{n-1} \\\n",
    "                && \\text{binomial expansion AKA \"FOILing\"} \\cr\n",
    "            &3. = \\frac{\\sum_{i=1}^n y_i^2 - \\sum_{i=1}^n 2y_i\\bar{y} + \\sum_{i=1}^n \\bar{y}^2}{n-1} \n",
    "                && \\text{summation operator is linear} \\cr\n",
    "            &4. = \\frac{\\sum_{i=1}^n y_i^2 - 2\\bar{y}\\sum_{i=1}^n y_i + n\\bar{y}^2}{n-1} \n",
    "                && \\text{constants can be factored out} \\cr\n",
    "            &5. = \\frac{\\sum_{i=1}^n y_i^2 - 2\\bar{y}n\\bar{y} + n\\bar{y}^2}{n-1} \n",
    "                && \\text{definition of summation} \\cr\n",
    "            &6. = \\frac{\\sum_{i=1}^n y_i^2 - n\\bar{y}^2}{n-1} \n",
    "                && \\text{arithmetic} \\cr\n",
    "            &7. = \\frac{n}{n-1} (\\overline{y^2} - \\bar{y})^2\n",
    "                && \\text{arithmetic} \\cr\n",
    "\\end{align*}\n",
    "\n",
    "### The general formula for the variance of a sum of random variables. \n",
    "\n",
    "Let *Y* be the sum of random variables $Y_1, Y_2, ... Y_k$. Assume centered variables for the sake of step 5, where we will want to replace each variable's deviation from its mean with a single symbol to make the multinomial expansion easier. This can be, in principle, any symbol; it seems most natural to just use the variable itself, and we often assume centered variables anyways. Note that there is absolutely no difference if I had used, say, $\\Delta_{Y_j}$ to represent the deviation of the *j*th variable from its mean. \n",
    "\n",
    "To get an intuition for the correct algorithm for multinomial expansion, draw a picture of rectangle with both unique side lengths partitioned into $Y_1, Y_2, ... Y_k$. Then, find the area of the rectangle, which is logically equal, of course, to finding $(Y_1 + Y_2 + ... Y_k)^2$. The rectangle is now comprised of smaller rectangles with areas $(Y_1*Y_1), (Y_2*Y_1) ... (Y_k*Y_1)$ going down the first column, $(Y_1*Y_2), (Y_2*Y_2) ... (Y_k*Y_2)$ going down the second, etc. We can thus visualize this as operation as follows: write out $(Y_1 + Y_2 + ... Y_k)^2$ as $(Y_1 + Y_2 + ... Y_k)*(Y_1 + Y_2 + ... Y_k)$. Starting with the first (or second; it doesn't matter) set of parentheses, take each term, multiply it by every term in the second set of parentheses, then add them up, and then do this for each term in the first set. Then, add up all the summed terms. This corresponds to summing up all of the items in the variance-covariance matrix. It was too time-consuming for me to draw a picture of all of this in Markdown, but DeVellis (2003) is a good, unintimidating visual representation of this sort of proof (which is really useful in general in the context of statistics). \n",
    "\n",
    "By the way, it may be useful to note that to sum all entries in a matrix, we can write it as a quadratic form. If we have centered variables, our covariance matrix $\\boldsymbol{\\Sigma}$ is simply $\\frac{1}{n-1}\\textbf{X}^t\\textbf{X}$, where $\\textbf{X}$ is the data matrix. Then, to sum all elements in that matrix, we write. $\\vec{1}^t\\boldsymbol{\\Sigma}\\vec{1}$. This proof won't use matrix properties, though. \n",
    "\n",
    " \\begin{align*}\n",
    "            &1. \\mathbb{V}[Y] = \\mathbb{E}[(Y - \\mu_Y)^2] && \\text{definition of variance} \\cr\n",
    "            &2. = \\mathbb{E}[([Y_1 + Y_2 + ... Y_k] - \\mu_Y)^2] && \\text{definition of variable Y} \\cr\n",
    "            &3. = \\mathbb{E}[([Y_1 + Y_2 + ... Y_k] - [\\mu_{Y_1} + \\mu_{Y_2} + ... \\mu_{Y_k}])^2] \\\n",
    "                && \\text{expectation operator is linear} \\cr\n",
    "            &4. = \\mathbb{E}[(Y_1 - \\mu_{Y_1} + Y_2 - \\mu_{Y_2} + ... Y_k - \\mu_{Y_k})^2]\n",
    "                && \\text{commutativity of addition} \\cr\n",
    "            &5. = \\mathbb{E}[(Y_1 + Y_2 + ... Y_k)^2]\n",
    "                && \\text{definition of centered variables} \\cr\n",
    "            &6. = \\mathbb{E}[Y_1^2 + (Y_2 * Y_1) + ... (Y_k * Y_1) + (Y_1 * Y_2) + Y_2^2 + (Y_3 * Y_2) ... ]\n",
    "                && \\text{picture the rectangle as mentioned above} \\cr\n",
    "            &7. = \\mathbb{E}[\\sum_{j=1}^k\\sum_{i=1}^k Y_i * Y_j]\n",
    "                && \\text{generalizing the pattern} \\cr\n",
    "            &8. = \\sum_{j=1}^k\\sum_{i=1}^k \\sigma^2_{Y_i, Y_j}\n",
    "                && \\text{definition of(co)variances} \\cr\n",
    "            &9. = \\sum_{j=1}^k \\sigma^2_{Y_j} + 2\\sum_{i>j}^k\\sum_{j=1}^k \\sigma^2_{Y_i, Y_j}\n",
    "                && \\text{covariance matrix is symmetric}\n",
    "        \\end{align*}\n",
    "\n",
    "Of course, we often work with variables&mdash;such as the sample means of women's education and that of men, $\\overline{Y}_F$ and $\\overline{Y}_{M}$&mdash;whose population covariance $\\sigma_{\\overline{Y}_F, \\overline{Y}_M}$ is equal to zero. Then, only the first term of line 9 above is relevant to the calculation of the variance of a random variable which is itself the summation or addition of other random variables. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}